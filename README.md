# Awesome-psychoinformatics

<b>Table of Contents</b>

## Analysis Tools in neuroimaging & neuropsychology
1. [fMRIPrep](https://fmriprep.org/en/stable/)


## Analysis Tools in behavior measurement
1. [Eyetracking fixation & saccade detection](https://github.com/psychoinformatics-de/remodnav)

## Databases of background knowledge
1. [Mayo Clinic Disease conditions](https://www.mayoclinic.org/diseases-conditions) [CN]https://www.mayoclinic.org/zh-hans/diseases-conditions)
2. [psychdb in Psychiatry](https://www.psychdb.com/home)
3. [Standford Encyclopedia of Philosophy](https://plato.stanford.edu/) <i>Cognitive Science, Philosophy of Mind in Analytical Philosophy, and more.</i>
4. [Internet Encyclopedia of Philosophy and its Authors | ISSN 2161-0002](https://iep.utm.edu/) <i>Peer-Reviewed Enceclopedia of Philosophy, might contain more stable resources.</i>
5. [The Merck Manual of Medical Information](https://www.merckmanuals.com/home)

## Connectome & functional connectome papers
1. [Hoy, J. L., & Farrow, K. (2025). The superior colliculus. Current Biology, 35(5), R164-R168.](https://www.cell.com/current-biology/abstract/S0960-9822(25)00052-1)

## Databases of data
1. [Neuosynth](https://neurosynth.org/)
2. [Neurosynth-compose](https://compose.neurosynth.org/)
3. [Neurovault](https://neurovault.org/)
4. [Connectome](https://www.humanconnectome.org/software/connectome-workbench)
5. [Functional Connectome](https://github.com/NotaCS/Functionnectome)
6. [Disconnectome](http://165.232.73.88/)
7. [BCBtoolkit](https://storage.googleapis.com/bcblabweb/index.html)
8. [OpenNeuro](https://openneuro.org/)
9. [brainlife](https://brainlife.io/about/)
10. [Clinica](https://aramislab.paris.inria.fr/clinica/docs/public/latest/)

## Databases in theory
1. [Buckner, R. L., Krienen, F. M., Castellanos, A., Diaz, J. C., & Yeo, B. T. (2011). The organization of the human cerebellum estimated by intrinsic functional connectivity. Journal of neurophysiology, 106(5), 2322-2345.](https://journals.physiology.org/doi/full/10.1152/jn.00339.2011)
2. [Delude, C. M. (2015). Deep phenotyping: the details of disease. Nature, 527(7576), S14-S15.](https://www.nature.com/articles/527S14a)
3. [Wright, J. T., & Herzberg, M. C. (2021). Science for the next century: deep phenotyping. Journal of dental research, 100(8), 785-789.](https://journals.sagepub.com/doi/full/10.1177/00220345211001850)

## Foundational brain-inspired neural network structures through MLP
1. [Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray, A., Tschaikowski, M., ... & Rus, D. (2022). Closed-form continuous-time neural networks. Nature Machine Intelligence, 4(11), 992-1003.](https://www.nature.com/articles/s42256-022-00556-7)
2. [Lechner, M., Hasani, R., Amini, A., Henzinger, T. A., Rus, D., & Grosu, R. (2020). Neural circuit policies enabling auditable autonomy. Nature Machine Intelligence, 2(10), 642-652.](https://www.nature.com/articles/s42256-020-00237-3) <i>liquid network</i>
3. [Marr, D., & Poggio, T. (1976). From understanding computation to understanding neural circuitry.](https://dspace.mit.edu/handle/1721.1/5782)
4. [Raju, R. V., Guntupalli, J. S., Zhou, G., Wendelken, C., Lázaro-Gredilla, M., & George, D. (2024). Space is a latent sequence: A theory of the hippocampus. Science Advances, 10(31), eadm8470.](https://www.science.org/doi/10.1126/sciadv.adm8470) <i>hipposcampus' role to prefrontal cortex [Frankland, P. W., & Bontempi, B. (2005). The organization of recent and remote memories. Nature reviews neuroscience, 6(2), 119-130.](https://www.nature.com/articles/nrn1607)</i>
5. [Stenlund, M. (2025). Introduction to Predictive Coding Networks for Machine Learning. arXiv preprint arXiv:2506.06332.](https://arxiv.org/abs/2506.06332) <i>Introduction of Predictive Coding Network, PCN. </i>
6. [Pinchetti, L., Qi, C., Lokshyn, O., Olivers, G., Emde, C., Tang, M., ... & Salvatori, T. (2024). Benchmarking Predictive Coding Networks--Made Simple. arXiv preprint arXiv:2407.01163.](https://openreview.net/forum?id=sahQq2sH5x) <i>Scalability issue of predictive coding network compared to BP for deep network; Former learning algorithm is called [Equilibrium Propagation](https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2017.00024/full)</i>; <i>Published as a conference paper at ICLR 2025</i>
7. [van Zwol, B., Jefferson, R., & Broek, E. L. (2024). Predictive Coding Networks and Inference Learning: Tutorial and Survey. arXiv preprint arXiv:2407.04117.](https://arxiv.org/abs/2407.04117) <i>predictive coding network</i>

## Information geometry
1. [Nielsen, F. (2020). An elementary introduction to information geometry. Entropy, 22(10), 1100.](https://www.mdpi.com/1099-4300/22/10/1100)


## Neural engeineering frameworks
1. Eliasmith, C. (2005). A unified approach to building and controlling spiking attractor networks. Neural computation, 17(6), 1276-1314. Translationable package: [Nengo](https://www.nengo.ai/)

## Neural manifold theories
1. [Barrett, L. F. (2017). The theory of constructed emotion: an active inference account of interoception and categorization. Social cognitive and affective neuroscience, 12(1), 1-23.](https://academic.oup.com/scan/article/12/1/1/2823712) <i>active inference account of emotion, Friston's related work</i>
2. [Behrens, T. E., Muller, T. H., Whittington, J. C., Mark, S., Baram, A. B., Stachenfeld, K. L., & Kurth-Nelson, Z. (2018). What is a cognitive map? Organizing knowledge for flexible behavior. Neuron, 100(2), 490-509.](https://www.cell.com/neuron/fulltext/S0896-6273(18)30856-0) <i>shows concepts and thoughts follow geometric, low-dimensional representations, supporting the reduced-dimension hypothesis</i>
3. [Benas, S., Fernandez, X., & Kropff, E. (2024). Modeled grid cells aligned by a flexible attractor. Elife, 12, RP89851.](https://elifesciences.org/articles/89851) <i>Using CAN (Continuous Attractor Networks) to model grid cell, aligned to the grid cell empirical finding by Gardner et al., 2022.</i>
4. [Carter, K. M., Raich, R., Finn, W. G., & Hero III, A. O. (2009). Fine: Fisher information nonparametric embedding. IEEE transactions on pattern analysis and machine intelligence, 31(11), 2093-2098.](https://ieeexplore.ieee.org/abstract/document/4815255/?casa_token=1VJcILKmxTsAAAAA:bcXXFAlCVgHqCMiueMgJdtSdn7IJ0omq9FWH215PRrsoQqbxnRf8Kv1ReI7NMx7vMG1NP0oF) <i>A term could be invented here, "scaler learning" or Metric Learning with Psychologist Stevens' contribution of Levels of Measurement: <b>Stevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677-680.</b></i>
5. [Claudi, F., & Branco, T. (2022). Differential geometry methods for constructing manifold-targeted recurrent neural networks. Neural Computation, 34(8), 1790-1811.](https://direct.mit.edu/neco/article-abstract/34/8/1790/111783/Differential-Geometry-Methods-for-Constructing) 
6. [Dabaghian, Y., Brandt, V. L., & Frank, L. M. (2014). Reconceiving the hippocampal map as a topological template. Elife, 3, e03476.](https://elifesciences.org/articles/03476)
7. [Friston, K. (2010). The free-energy principle: a unified brain theory?. Nature reviews neuroscience, 11(2), 127-138.](https://www.fil.ion.ucl.ac.uk/~karl/The%20free-energy%20principle%20A%20unified%20brain%20theory.pdf)
8. [Gorban, A. N., & Tyukin, I. Y. (2018). Blessing of dimensionality: mathematical foundations of the statistical physics of data. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 376(2118), 20170237.](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2017.0237)
9. [Har-Shemesh, O., Quax, R., Lansing, J. S., & Sloot, P. M. (2020). Questionnaire data analysis using information geometry. Scientific Reports, 10(1), 8633.](https://www.nature.com/articles/s41598-020-63760-8)
10. [Khona, M., & Fiete, I. R. (2022). Attractor and integrator networks in the brain. Nature Reviews Neuroscience, 23(12), 744-766.](https://www.nature.com/articles/s41583-022-00642-0)
11. [Langdon, C., Genkin, M., & Engel, T. A. (2023). A unifying perspective on neural manifolds and circuits for cognition. Nature Reviews Neuroscience, 24(6), 363-377.](https://www.nature.com/articles/s41583-023-00693-x) <i>Unify the neural circuit/motif? with the neural/cognitive manifold</i>
12. [Lei, N., An, D., Guo, Y., Su, K., Liu, S., Luo, Z., ... & Gu, X. (2020). A geometric understanding of deep learning. Engineering, 6(3), 361-374.](https://www.sciencedirect.com/science/article/pii/S2095809919302279) <i>manifold distribution principle</i>
13. [Li, Y., Michaud, E. J., Baek, D. D., Engels, J., Sun, X., & Tegmark, M. (2024). The Geometry of Concepts: Sparse Autoencoder Feature Structure. arXiv preprint arXiv:2410.19750.](https://arxiv.org/abs/2410.19750) <i>semantic maps also found in the Nature article "[Natural speech reveals the semantic maps that tile human cerebral cortex](https://www.nature.com/articles/nature17637)"</i>
14. [Spisak, T., & Friston, K. (2025). Self-orthogonalizing attractor neural networks emerging from the free energy principle. arXiv preprint arXiv:2505.22749.](https://arxiv.org/abs/2505.22749) <i>continuous attractor network and free energy principle, FEP, and [the link of the interactive paper plus the code](https://pni-lab.github.io/fep-attractor-network/)</i>
15. [Perich, M. G., Narain, D., & Gallego, J. A. (2025). A neural manifold view of the brain. Nature Neuroscience, 1-16.](https://www.nature.com/articles/s41593-025-02031-z)
16. [Tuerlinckx, F. (2024). A geometrical perspective on parametric psychometric models. arXiv preprint arXiv:2410.12450.](https://arxiv.org/abs/2410.12450)
17. [Whittington, J. C., Muller, T. H., Mark, S., Chen, G., Barry, C., Burgess, N., & Behrens, T. E. (2020). The Tolman-Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell, 183(5), 1249-1263.](https://www.sciencedirect.com/science/article/pii/S009286742031388X)
18. [MISC-Math list-Awesome maths resources in neural geometry](https://github.com/moomoofarm1/awesome-neural-geometry)

## Neural manifold methods (extraction)
1. [Chung, S., & Abbott, L. F. (2021). Neural population geometry: An approach for understanding biological and artificial neural networks. Current opinion in neurobiology, 70, 137-144.](https://www.sciencedirect.com/science/article/pii/S0959438821001227)  <i>It defines the field. [notes of this, illustraing math tools, Manifold capacity theory, Representational similarity and RDMs, Extrinsic/intrinsic curvature, Information geometry, Control-theoretic reductions, Optimal transport distances; This author defines neural manifold is from the neural populations, usually 1k+.](https://www.emergentmind.com/topics/neural-population-geometry)</i>
2. [Li, Y., Michaud, E. J., Baek, D. D., Engels, J., Sun, X., & Tegmark, M. (2025). The geometry of concepts: Sparse autoencoder feature structure. Entropy, 27(4), 344.](https://www.mdpi.com/1099-4300/27/4/344) <i>umap visualization in the embedding space</i>
3. [Meilă, M., & Zhang, H. (2024). Manifold learning: What, how, and why. Annual Review of Statistics and Its Application, 11(1), 393-417.](https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-040522-115238?crawler=true)
4. [Mitchell-Heggs, R., Prado, S., Gava, G. P., Go, M. A., & Schultz, S. R. (2023). Neural manifold analysis of brain circuit dynamics in health and disease. Journal of computational neuroscience, 51(1), 1-21.](https://link.springer.com/article/10.1007/s10827-022-00839-3)
5. [Schneider, S., Lee, J. H., & Mathis, M. W. (2023). Learnable latent embeddings for joint behavioural and neural analysis. Nature, 617(7960), 360-368.](https://www.nature.com/articles/s41586-023-06031-6) <i>CEBRA, quote: Everyone loves this method (aka neural manifold) that looks fancy with 2D/3D graphs, from the early jPCA/dPCA/TDR to TDA, and then to the incredibly beautiful labeled training CEBRA. from [one peer on Zhihu](https://www.zhihu.com/question/1968737587788231218/answer/1968837762762400954); Topological data analysis</i>
6. [Stringer, C., & Pachitariu, M. (2024). Analysis methods for large-scale neuronal recordings. Science, 386(6722), eadp7429.](https://www.science.org/doi/10.1126/science.adp7429)
7. [Wang, E. Y., Fahey, P. G., Ding, Z., Papadopoulos, S., Ponder, K., Weis, M. A., ... & Tolias, A. S. (2025). Foundation model of neural activity predicts response to new stimulus types. Nature, 640(8058), 470-477.](https://www.nature.com/articles/s41586-025-08829-y) <i>The last layer of the Vision model. Used Gaussian-splatting-like method to model Receptive Field, to extract object (semantic) information.</i>
8. [Zhang, J., & Hästö, P. (2006). Statistical manifold as an affine space: A functional equation approach. Journal of Mathematical Psychology, 50(1), 60-65.](https://www.sciencedirect.com/science/article/pii/S0022249605000738) <i>The manifold transformed which is easier for mathematical calculation</i>

## Neural manifold existence from empirical findings
See the articles in a different [list](https://github.com/moomoofarm1/awesome_neural_manifolds).

## Brain-inspired computing (birds-eye view, or Macro-level, i.e., the lobe level)
This section includes Predictive Coding Network (PCN, i.e., energy-based models) since the PCN is based on the Bayesian Brain Hypothesis which is not a neuromorphic model but a biologically-plausible model.
1. [Domingos, P. (2012). A few useful things to know about machine learning. Communications of the ACM, 55(10), 78-87.](https://dl.acm.org/doi/abs/10.1145/2347736.2347755)
2. [Doya, K. (2000). Complementary roles of basal ganglia and cerebellum in learning and motor control. Current opinion in neurobiology, 10(6), 732-739.](https://www.sciencedirect.com/science/article/pii/S0959438800001537)
3. [Chandra, S., Sharma, S., Chaudhuri, R., & Fiete, I. (2025). Episodic and associative memory from spatial scaffolds in the hippocampus. Nature, 638(8051), 739-751.](https://www.nature.com/articles/s41586-024-08392-y)
4. [Furlong, P. M., & Eliasmith, C. (2023). Bridging cognitive architectures and generative models with vector symbolic algebras. In Proceedings of the AAAI Symposium Series (Vol. 2, No. 1, pp. 262-271).](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27686) <i>SPAUN</i>
5. [Friston, K. (2009). The free-energy principle: a rough guide to the brain?. Trends in cognitive sciences, 13(7), 293-301.](https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(09)00117-X) <i>Friston's paper on the free-energy principle</i>
6. [Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., & Pezzulo, G. (2016). Active inference and learning. Neuroscience & Biobehavioral Reviews, 68, 86.](https://www.sciencedirect.com/science/article/pii/S0149763416301336)
7. [Hafner, D., Pasukonis, J., Ba, J. et al. Mastering diverse control tasks through world models. Nature 640, 647–653 (2025). https://doi.org/10.1038/s41586-025-08744-2](https://www.nature.com/articles/s41586-025-08744-2)
8. [Jirsa, V. (2020). Structured flows on manifolds as guiding concepts in brain science. Selbstorganisation–ein Paradigma für die Humanwissenschaften: Zu Ehren von Günter Schiepek und seiner Forschung zu Komplexität und Dynamik in der Psychologie, 89-102.](https://link.springer.com/chapter/10.1007/978-3-658-29906-4_6)
9. [Jirsa, V., & Sheheitli, H. (2022). Entropy, free energy, symmetry and dynamics in the brain. Journal of Physics: Complexity, 3(1), 015007.](https://iopscience.iop.org/article/10.1088/2632-072X/ac4bec/meta)
10. [Lai, M., Go, K., Li, Z., Kröger, T., Schaal, S., Allen, K., & Scholz, J. (2025). RoboBallet: Planning for multirobot reaching with graph neural networks and reinforcement learning. Science Robotics, 10(106), eads1204.](https://www.science.org/doi/10.1126/scirobotics.ads1204) <i>Graph network as the policy network (as well as being the space representation, another implementation of the Tolman-Eichenbaum machine</i>
11. [LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A tutorial on energy-based learning. Predicting structured data, 1(0).](https://www.researchgate.net/profile/Marcaurelio-Ranzato/publication/216792742_A_Tutorial_on_Energy-Based_Learning/links/0912f50c6862425435000000/A-Tutorial-on-Energy-Based-Learning.pdf) <i>Lecun, EMB, Energy-Based Models, akin to Jirsa's work, for maybe reinforcement learning?</i>
12. [Liu, B., Li, X., Zhang, J., Wang, J., He, T., Hong, S., ... & Wu, C. (2025). Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems. arXiv preprint arXiv:2504.01990.](https://arxiv.org/abs/2504.01990) <i>brain-inspired agents building</i>
13. [Millidge, B., Salvatori, T., Song, Y., Bogacz, R., & Lukasiewicz, T. (2022). Predictive coding: Towards a future of deep learning beyond backpropagation?. arXiv preprint arXiv:2202.09467.](https://arxiv.org/abs/2202.09467) <i>PCN is equivalent to BP algorithm.</i>
14. [Millidge, B., Song, Y., Salvatori, T., Lukasiewicz, T., & Bogacz, R. (2022). A theoretical framework for inference and learning in predictive coding networks. arXiv preprint arXiv:2207.12316.](https://arxiv.org/abs/2207.12316) <i>MLP-based active inference and predictive coding</i>
15. [Millidge, B., Tschantz, A., & Buckley, C. L. (2022). Predictive coding approximates backprop along arbitrary computation graphs. Neural Computation, 34(6), 1329-1368.](https://direct.mit.edu/neco/article-abstract/34/6/1329/110646/Predictive-Coding-Approximates-Backprop-Along) <i>ChatGPT: It provides the general recipe for mapping differentiable models to PC graphs; supports the idea that an ANN’s computation (including its last hidden) can be embedded in or interfaced with a PCN for iterative inference.</i>
16. [Qi, C., Forasassi, M., Lukasiewicz, T., & Salvatori, T. (2025). Towards the Training of Deeper Predictive Coding Neural Networks. arXiv preprint arXiv:2506.23800.](https://arxiv.org/html/2506.23800v3) <i>Solution to the paper: Pinchetti et al., 2024, Benchmarking Predictive Coding Networks--Made Simple. For the drop of accuracy for over 5 layers of PCN.</i>
17. [Ramstead, M. J., Sakthivadivel, D. A., Heins, C., Koudahl, M., Millidge, B., Da Costa, L., ... & Friston, K. J. (2023). On Bayesian mechanics: a physics of and by beliefs. Interface Focus, 13(3), 20220029.](https://royalsocietypublishing.org/rsfs/article/13/3/20220029/89434/On-Bayesian-mechanics-a-physics-of-and-by)
18. [Rosenbaum, R. (2022). On the relationship between predictive coding and backpropagation. Plos one, 17(3), e0266102.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0266102) <i>PCN is similar to BP algorithm.</i> [Link to the git repo of Torch2PC](https://github.com/RobertRosenbaum/Torch2PC)
19. [Ren, J., & Xia, F. (2024). Brain-inspired Artificial Intelligence: A Comprehensive Review. arXiv preprint arXiv:2408.14811.](https://arxiv.org/abs/2408.14811)
20. [Salvatori, T., Mali, A., Buckley, C. L., Lukasiewicz, T., Rao, R. P., Friston, K., & Ororbia, A. (2025). A survey on neuro-mimetic deep learning via predictive coding. Neural Networks, 108161.](https://www.sciencedirect.com/science/article/pii/S089360802501041X)
21. [Song, Y., Millidge, B., Salvatori, T., Lukasiewicz, T., Xu, Z., & Bogacz, R. (2024). Inferring neural activity before plasticity as a foundation for learning beyond backpropagation. Nature neuroscience, 27(2), 348-358.](https://www.nature.com/articles/s41593-023-01514-1) <i>Idea: PCN receives signals from LLM; ChatGPT: This shows that if you set PCN weights to the corresponding ANN weights, the PCN produces the same feedforward prediction as the ANN (inputs clamped, outputs free). This is the most explicit top-journal statement enabling an interface: you can map an ANN to a PCN, then (in practice) clamp/top-down seed the PCN with ANN states.</i>
22. [Tscshantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2023). Hybrid predictive coding: Inferring, fast and slow. PLoS computational biology, 19(8), e1011280.](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011280) <i>Hybrid, ANN, PCN</i>
23. [van Zwol, B. Predictive Coding Graphs are a Superset of Feedforward Neural Networks. In The First Workshop on NeuroAI@ NeurIPS2024.](https://openreview.net/forum?id=J36z3R0sNq)
24. [Zahid, U., Guo, Q., & Fountas, Z. (2023). Predictive coding as a neuromorphic alternative to backpropagation: a critical evaluation. Neural Computation, 35(12), 1881-1909.](https://direct.mit.edu/neco/article/35/12/1881/117833) <i>cited words: standard formulation … PC … has benefited from its interpretation as a variational Bayes algorithm for learning and inference in latent hierarchical models; one of the strengths of standard PC formulations has historically been … that latents hierarchically higher in the cortex have a non-linearly mixing and modulatory effect … congruent with what we know about cortical anatomy; Comments: these words indicate the benefits of PCN: 1. include uncertainty/probability in hidden states, 2. show the information flow / pipeline explicity according to brain-inspired/brain-like mechanisms.</i>

## Neuromath
also, symbolic regression in clinics
0. [Book-Mathematical-Foundation-of-Reinforcement-Learning](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning)
1. [Bazargani, M. H., Urbas, S., & Friston, K. (2025). Brain in the Dark: Design Principles for Neuromimetic Inference under the Free Energy Principle. arXiv preprint arXiv:2502.08860.](https://arxiv.org/abs/2502.08860) <i>Demistify the FEP to pytorch in the github [repo](https://github.com/MLDawn/PC-network-NeurIPs-2024). </i>
2. [La Cava, W. G., Lee, P. C., Ajmal, I., Ding, X., Solanki, P., Cohen, J. B., ... & Herman, D. S. (2023). A flexible symbolic regression method for constructing interpretable clinical prediction models. NPJ Digital Medicine, 6(1), 107.](https://www.nature.com/articles/s41746-023-00833-8)

## Neuromorphic computing (cognition synthesis, including non-human species)
This section draws inspirations from functional circuits in [functional connectomics](https://arxiv.org/abs/2211.12935) research to use artificial neural networks to synthesis the target brain/psychological function. Theoretical underpinning: main functional networks in the brain [Uddin et al., 2019](https://link.springer.com/article/10.1007/s10548-019-00744-6), such as task positive network, and default brain network. The goal is to form a system similar to period table in chemistry to construct target functions?
From the machine learning community, similar concepts exist: neural modular network, or broader, modular learning. Modular learning is about (from LLM) structuring systems into components that specialize and possibly interact through shared or separate representations; Often, each module learns or uses its own latent representation, or they coordinate via a shared latent space. In the curator's words, latent representation learning such as Lecun's JEPA framework (a path towards autonomous machine intelligence).
1. [Andreas, J., Rohrbach, M., Darrell, T., & Klein, D. (2016). Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 39-48).](https://openaccess.thecvf.com/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html)
2. [Fashandi, H. (2023). Neural module networks: A review. Neurocomputing, 552, 126518.](https://www.sciencedirect.com/science/article/pii/S0925231223006410) <i>neural modular networks</i>
3. [Thiebaut de Schotten, M., & Forkel, S. J. (2022). The emergent properties of the connected brain. Science, 378(6619), 505-510.](https://www.science.org/doi/abs/10.1126/science.abq2591) <i>The engineering field drawing insights from neural motifs. Mainly from <b>Functional connectomics</b>.</i> 
4. [Vaxenburg, R., Siwanowicz, I., Merel, J., Robie, A. A., Morrow, C., Novati, G., ... & Turaga, S. C. (2025). Whole-body physics simulation of fruit fly locomotion. Nature, 1-3.](https://www.nature.com/articles/s41586-025-09029-4)
5. [Zeng, Y., Zhao, Y., Bai, J., & Xu, B. (2018). Toward robot self-consciousness (ii): brain-inspired robot bodily self model for self-recognition. Cognitive Computation, 10, 307-320.](https://link.springer.com/article/10.1007/s12559-017-9505-1)
6. [Zeng, Y., Zhao, D., Zhao, F., Shen, G., Dong, Y., Lu, E., ... & Bi, W. (2023). Braincog: A spiking neural network based, brain-inspired cognitive intelligence engine for brain-inspired ai and brain simulation. Patterns, 4(8).](https://www.cell.com/patterns/fulltext/S2666-3899(23)00144-7)
7. [Zou, Z., Zhao, R., Wu, Y., Yang, Z., Tian, L., Wu, S., ... & Shi, L. (2020). A hybrid and scalable brain-inspired robotic platform. Scientific reports, 10(1), 18160.](https://www.nature.com/articles/s41598-020-73366-9)

## Synthetic psychology (machine to form psychological functions, methods only)
1. [Chandra, S., Sharma, S., Chaudhuri, R., & Fiete, I. (2025). Episodic and associative memory from spatial scaffolds in the hippocampus. Nature, 638(8051), 739-751.](https://www.nature.com/articles/s41586-024-08392-y) <i>ANN models of memory. Comments by an unknown peer in computational neuroscience: Your idea is more like below. In addition to psychology and mathematics, a third approach has recently emerged, which is the perspective of AI. A very beautiful recent work, Episodic and associative memory from spatial scaffolds in the hippocampus, is somewhat similar. The emergence of this article definitely shows that the third approach has also become one of the mainstream in recent years. However, because it is too similar to alchemy, it is not widely accepted by traditional neurosci scholars.</i>
2. [Dawson, M. R. (2002, August). From embodied cognitive science to synthetic psychology. In Proceedings First IEEE International Conference on Cognitive Informatics (pp. 13-22). IEEE.](https://ieeexplore.ieee.org/abstract/document/1039276) <i>foundational work of this field</i>
3. [Prescott, T. J., & Camilleri, D. (2018). The synthetic psychology of the self. In Cognitive architectures (pp. 85-104). Cham: Springer International Publishing.](https://link.springer.com/chapter/10.1007/978-3-319-97550-4_7)
4. [Soni, N., Matero, M., Balasubramanian, N., & Schwartz, H. A. (2022, May). Human Language Modeling. In Findings of the Association for Computational Linguistics: ACL 2022 (pp. 622-636).](https://aclanthology.org/2022.findings-acl.52/) <i>HaRT framework.</i>

## Synthetic psychopathology
1. [Cao, Z., Xiao, X., Zhao, Y., Jiang, Y., Xie, C., Paillère-Martinot, M. L., ... & Zhu, C. (2023). Targeting the pathological network: Feasibility of network-based optimization of transcranial magnetic stimulation coil placement for treatment of psychiatric disorders. Frontiers in Neuroscience, 16, 1079078.](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.1079078/full)
2. [Pio-Lopez, L., Kuchling, F., Tung, A., Pezzulo, G., & Levin, M. (2022). Active inference, morphogenesis, and computational psychiatry. Frontiers in computational neuroscience, 16, 988977.](https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2022.988977/full)
3. [Powers, A., Angelos, P. A., Bond, A., Farina, E., Fredericks, C., Gandhi, J., ... & Benrimoh, D. (2025). A computational account of the development and evolution of psychotic symptoms. Biological Psychiatry, 97(2), 117-127.](https://www.sciencedirect.com/science/article/pii/S0006322324015841)

## MISC-AI notes
1. [ALPHA-FACTORY V1: Multi-Agent AGENTIC α-AGI World-Model, Quebec AI in Montreal](https://github.com/MontrealAI/AGI-Alpha-Agent-v0/blob/main/alpha_factory_v1/demos/alpha_asi_world_model/Alpha_ASI_World_Model.pdf)
2. [Carson, J. D., & Reisizadeh, A. (2025). A Statistical Physics of Language Model Reasoning. arXiv preprint arXiv:2506.04374.](https://arxiv.org/abs/2506.04374)
3. [Friston, K. J., Ramstead, M. J., & Sakthivadivel, D. A. (2024). A framework for the use of generative modelling in non-equilibrium statistical mechanics. arXiv preprint arXiv:2406.11630.](https://arxiv.org/abs/2406.11630) <i>Friston, active inference</i>
4. [GitZH-Chen awesome riemanian deep learning](https://github.com/GitZH-Chen/Awesome-Riemannian-Deep-Learning)
5. [google-gemini/gemini-fullstack-langgraph-quickstart](https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart) <i>Deep research, LLM, LangGraph agent</i>
6. [He, H., & Zhong, X. (2018). Learning without external reward [research frontier]. IEEE Computational Intelligence Magazine, 13(3), 48-54.](https://arxiv.org/abs/2505.19590)
7. [How ChatGPT Memory Works](https://macro.com/app/md/54115a42-3409-4f5b-9120-f144d3ecd23a)
8. [Hugging Face Team. (n.d.). Smol training playbook: Training compass — Why, what, how. Hugging Face. https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#training-compass-why--what--how](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#introduction)
9. [Huth, A. G., De Heer, W. A., Griffiths, T. L., Theunissen, F. E., & Gallant, J. L. (2016). Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532(7600), 453-458.](https://www.nature.com/articles/nature17637) <i>Distributed representation of language in the human cerebral cortex</i>
10. [La Cava, W. G., Lee, P. C., Ajmal, I., Ding, X., Solanki, P., Cohen, J. B., ... & Herman, D. S. (2023). A flexible symbolic regression method for constructing interpretable clinical prediction models. NPJ Digital Medicine, 6(1), 107.](https://www.nature.com/articles/s41746-023-00833-8)
11. [Libro-plus, the benchmark set for Vision-Language-Action models, VLA](https://github.com/sylvestf/LIBERO-plus)
12. [neurreps: awesome-neural-geometry](https://github.com/neurreps/awesome-neural-geometry)
13. [Ororbia, A., Friston, K., & Rao, R. P. (2025). Meta-Representational Predictive Coding: Biomimetic Self-Supervised Learning. arXiv preprint arXiv:2503.21796.](https://arxiv.org/abs/2503.21796) <i>Pure self-supervised predict coding network</i>
14. [Parthasarathy, V. B., Zafar, A., Khan, A., & Shahid, A. (2024). The ultimate guide to fine-tuning llms from basics to breakthroughs: An exhaustive review of technologies, research, best practices, applied research challenges and opportunities. arXiv preprint arXiv:2408.13296.](https://arxiv.org/abs/2408.13296)
15. [Qela, B., Damiani, S., De Santis, S., Groppi, F., Pichiecchio, A., Asteggiano, C., ... & Fusar-Poli, L. (2025). Predictive coding in neuropsychiatric disorders: A systematic transdiagnostic review. Neuroscience & Biobehavioral Reviews, 106020.](https://www.sciencedirect.com/science/article/pii/S014976342500020X)<i>Predictive coding network, PCN, in mental health, review, 2025</i>
16. [robert-mcdermott/ai-knowledge-graph](https://github.com/robert-mcdermott/ai-knowledge-graph) <i>LLM + knowledge graph</i>
17. [Salvatori, T., Mali, A., Buckley, C. L., Lukasiewicz, T., Rao, R. P., Friston, K., & Ororbia, A. (2023). Brain-inspired computational intelligence via predictive coding. arXiv preprint arXiv:2308.07870, 13.](https://www.researchgate.net/publication/373142044_Brain-Inspired_Computational_Intelligence_via_Predictive_Coding) <i>Friston, predictive coding network, MNIST</i>
18. [Sharma, U., & Kaplan, J. (2020). A neural scaling law from the dimension of the data manifold. arXiv preprint arXiv:2004.10802.](https://arxiv.org/abs/2004.10802) <i>Manifold, Scaling law of LLM/AI, manifold hypothesis of LLM</i>
19. [Stenlund, M. (2025). Introduction to Predictive Coding Networks for Machine Learning. arXiv preprint arXiv:2506.06332.](https://arxiv.org/abs/2506.06332)<i>Predictive Coding Network Pytorch tutorial, PCN, 2025</i>
20. [van Zwol, B. Predictive Coding Graphs are a Superset of Feedforward Neural Networks. In The First Workshop on NeuroAI@ NeurIPS2024.](https://openreview.net/forum?id=J36z3R0sNq)
21. [Wang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., ... & Shen, Y. (2025). Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571.](https://arxiv.org/abs/2504.20571)
22. [Xue, C., Liu, W., Xie, S., Wang, Z., Li, J., Peng, X., ... & Tao, D. (2025). Omniforce: on human-centered, large model empowered and cloud-edge collaborative AutoML system. npj Artificial Intelligence, 1(1), 3.](https://www.nature.com/articles/s44387-025-00002-0)
23. [Yeo, W., Kim, K., Jeong, S., Baek, J., & Hwang, S. J. (2025). UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities. arXiv preprint arXiv:2504.20734.](https://arxiv.org/abs/2504.20734)
24. [Zheng, K., Chen, Y., Mao, H., Liu, M. Y., Zhu, J., & Zhang, Q. (2024). Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908.](https://www.researchgate.net/publication/383754181_Masked_Diffusion_Models_are_Secretly_Time-Agnostic_Masked_Models_and_Exploit_Inaccurate_Categorical_Sampling)

## Own-contributed tools
1. [Topics package in R](https://cran.r-project.org/web/packages/topics/index.html)
2. [r-text package in R](https://r-text.org/)
3. [psyaitools in python mainly for psychoacoustics and AI](https://pypi.org/project/psyaitools/)


## Assess/Prediction(Prognostic) models
1. [The Language-Based Assessment Model (L-BAM) Library](https://r-text.org/articles/LBAM.html), contributed by the curator of this list.

## Miscellaneous
### Demographic-level embeddings and databases
<i>Though from sociology, this might be interesting to social psychologists.</i>
1. [Savcisens, G., Eliassi-Rad, T., Hansen, L. K., Mortensen, L. H., Lilleholt, L., Rogers, A., ... & Lehmann, S. (2024). Using sequences of life-events to predict human lives. Nature Computational Science, 4(1), 43-56.](https://www.nature.com/articles/s43588-023-00573-5)

### Meme
1. [Taniguchi, T., Takagi, S., Otsuka, J., Hayashi, Y., & Hamada, H. T. (2025). Collective predictive coding as model of science: Formalizing scientific activities towards generative science. Royal Society Open Science, 12(6), 241678.](https://royalsocietypublishing.org/rsos/article/12/6/241678/235366/Collective-predictive-coding-as-model-of-science) <i>Explain the cultural motive, meme.</i>


